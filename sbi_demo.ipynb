{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "442e1829",
   "metadata": {},
   "source": [
    "# Background\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350aad2d",
   "metadata": {},
   "source": [
    "# Prior and Simulations\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fad98d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glass_sim import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9defd824",
   "metadata": {},
   "source": [
    "## 1. Prior Parameters: $\\bm{\\theta}\\sim P(\\bm{\\theta})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "812b4a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform Samples:\n",
      " [[0.77376549 0.21837809 0.03078414]\n",
      " [0.72529927 0.37826868 0.03918482]\n",
      " [0.78668682 0.27350417 0.04972736]\n",
      " ...\n",
      " [0.71647135 0.35010276 0.03939765]\n",
      " [0.69206261 0.37843294 0.04189131]\n",
      " [0.71997984 0.34582458 0.0382579 ]]\n",
      "Normal Samples:\n",
      " [[0.689695   0.28421681 0.03304922]\n",
      " [0.65800537 0.15767512 0.03825492]\n",
      " [0.65471729 0.17705651 0.04308088]\n",
      " ...\n",
      " [0.78054443 0.25200362 0.04095189]\n",
      " [0.78159793 0.33429954 0.0394831 ]\n",
      " [0.68977909 0.27827112 0.02927799]]\n"
     ]
    }
   ],
   "source": [
    "def prior_param_samples(h_range, Oc_range, Ob_range, type, n_samples):\n",
    "    assert isinstance(h_range, tuple) and len(h_range) == 2, \"h_range must be a tuple of (min, max)\"\n",
    "    assert isinstance(Oc_range, tuple) and len(Oc_range) == 2, \"Oc_range must be a tuple of (min, max)\"\n",
    "    assert isinstance(Ob_range, tuple) and len(Ob_range) == 2, \"Ob_range must be a tuple of (min, max)\"\n",
    "    assert type in [\"uniform\", \"normal\"], \"type must be either 'uniform' or 'normal'\"\n",
    "    assert isinstance(n_samples, int) and n_samples > 0, \"n_samples must be a positive integer\"\n",
    "    if type == \"uniform\":\n",
    "        h_samples = np.random.uniform(h_range[0], h_range[1], n_samples)\n",
    "        Oc_samples = np.random.uniform(Oc_range[0], Oc_range[1], n_samples)\n",
    "        Ob_samples = np.random.uniform(Ob_range[0], Ob_range[1], n_samples)\n",
    "        samples = np.vstack((h_samples, Oc_samples, Ob_samples)).T\n",
    "    elif type == \"normal\":\n",
    "        cov = np.diag([h_range[1] - h_range[0], Oc_range[1] - Oc_range[0], Ob_range[1] - Ob_range[0]])**2 / 12\n",
    "        mean = [(h_range[0] + h_range[1]) / 2, (Oc_range[0] + Oc_range[1]) / 2, (Ob_range[0] + Ob_range[1]) / 2]\n",
    "        samples = np.random.multivariate_normal(mean, cov, n_samples)\n",
    "    return samples\n",
    "\n",
    "\n",
    "h_range = (0.6, 0.8)\n",
    "Oc_range = (0.2, 0.4)\n",
    "Ob_range = (0.03, 0.05)\n",
    "n_samples = 1000\n",
    "uniform_samples = prior_param_samples(h_range, Oc_range, Ob_range, \"uniform\", n_samples)\n",
    "print(\"Uniform Samples:\\n\", uniform_samples)\n",
    "normal_samples = prior_param_samples(h_range, Oc_range, Ob_range, \"normal\", n_samples)\n",
    "print(\"Normal Samples:\\n\", normal_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca10d40",
   "metadata": {},
   "source": [
    "##  2. Simulated Data: $\\bm{x}|\\bm{\\theta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ac41d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_samples = []\n",
    "cls_samples = []\n",
    "\n",
    "for i in range(uniform_samples.shape[0]):\n",
    "    cosmology = {}\n",
    "    cosmology['h'], cosmology['Oc'], cosmology['Ob'] = uniform_samples[i] # unpacking\n",
    "    sim = lensing_cls_sim(cosmology)\n",
    "    param_samples.append(sim['params'])\n",
    "    cls_samples.append(sim['cls'])\n",
    "param_samples = np.array(param_samples)\n",
    "cls_samples = np.array(cls_samples) \n",
    "\n",
    "print(param_samples)\n",
    "print(cls_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3914b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('sbi_demo_data.pkl', 'wb') as f:\n",
    "    # pickle.dump({'params': param_samples, 'cls': cls_samples}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4ba422",
   "metadata": {},
   "source": [
    "# Compression\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5d724d",
   "metadata": {},
   "source": [
    "## Linear compression methods\n",
    "Let $\\{\\mathbf{x}_1,...,\\mathbf{x}_N\\}$ be the set of simulated data with corresponding.  \n",
    "$$\n",
    "\\tilde{\\mathbf{x}}_{m\\times1} = T\\;\\mathbf{x}_{n\\times1}\n",
    "$$\n",
    "$T$ can be obtained via:\n",
    "\n",
    "1. Canoncical correlation analysis\n",
    "    - Maximises the correlation between the parameters and data (also could be written as maximising the mutual information).\n",
    "    - Use canonical data vectors as compressed data\n",
    "1. MOPED\n",
    "    - Assumes Gaussian likelihood surface and preserves the Fisher Information, as a result:\n",
    "        - requires fiducial cosmology simulations for covariance estimation\n",
    "        - requires additional simulation cost for mean derivative estimation\n",
    "1. e-MOPED \n",
    "    - Park, M., Gatti, M., & Jain, B. (2025). Dimensionality reduction techniques for statistical inference in cosmology.\n",
    "    - Define new data space as a transform of the parameter space centred at the mean parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e264e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cca(param_samples, cls_samples):\n",
    "    '''\n",
    "    Compute the sampled parameter auto covariance, simulated data vector auto covariance and the parameter-data vector cross covariance\n",
    "    Methodology as per Park, M., Gatti, M., & Jain, B. (2025). Dimensionality reduction techniques for statistical inference in cosmology.\n",
    "    '''\n",
    "    cov = np.cov(params_samples.T, cls_samples.T)\n",
    "    params_count = param_samples.shape[1]\n",
    "    cp = cov[:params_count,:params_count]\n",
    "    cd = cov[params_count:,params_count:]\n",
    "    cpd = cov[:params_count,params_count:]\n",
    "\n",
    "    cl = cpd.T@np.linalg.inv(cp)@cpd\n",
    "\n",
    "    canon_corr, canon_vecs = scipy.linalg.eigh(cd, cd - cl)\n",
    "    \n",
    "    return {'params': params_samples, 'compressed_cls': cla_samples@canon_vecs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9dbc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moped(param_samples, cls_samples, fiducial_cls_samples, perturbed_param_delta, perturbed_cls_samples):\n",
    "    '''\n",
    "    fiducial_cls used for covariance computation\n",
    "    perturbed_cls used for mean derivative computation\n",
    "    '''\n",
    "    cov_matrix = np.cov(fiducial_cls_samples.T)\n",
    "\n",
    "    deriv = []\n",
    "    for delta in perturbed_param_delta:\n",
    "        deriv.append(np.mean(perturbed_cls_samples - fiducial_cls_samples, axis=0)/delta) #first order finite difference\n",
    "\n",
    "    compression_matrix = np.zeros((param_samples.shape[1], cls_samples.shape[1])) # initialize compression matrix d_params x d_cls\n",
    "\n",
    "    for i in range(param_samples.shape[1]):\n",
    "        invcov_deriv = np.linalg.solve(cov_matrix, deriv[i])\n",
    "        if i>0:\n",
    "            coefs = transf_matrix[:i,:]@deriv[:i]\n",
    "            transf_matrix[i,:]= (invcov_deriv - transf_matrix[:i,:].T@coefs)/np.sqrt(deriv[i]@invcov_deriv- np.sum(coefs**2))\n",
    "        else:\n",
    "            transf_matrix[i,:]= (invcov_deriv)/np.sqrt(deriv[i]@invcov_deriv)\n",
    "\n",
    "    compressed_cls = compression_matrix@cls_samples.T\n",
    "    \n",
    "    return {'params': param_samples, 'compressed_cls': compressed_cls}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea3abead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:00, 35.06it/s]\n",
      "16it [00:00, 37.21it/s]\n",
      "16it [00:00, 35.32it/s]\n",
      "16it [00:00, 37.62it/s]\n",
      "16it [00:00, 37.11it/s]\n",
      "16it [00:00, 36.24it/s]\n",
      "16it [00:00, 37.22it/s]\n",
      "16it [00:00, 36.07it/s]\n",
      "16it [00:00, 35.90it/s]\n",
      "16it [00:00, 36.23it/s]\n",
      "16it [00:00, 35.85it/s]\n",
      "16it [00:00, 36.28it/s]\n",
      "16it [00:00, 36.87it/s]\n",
      "16it [00:00, 37.09it/s]\n",
      "16it [00:00, 36.64it/s]\n",
      "16it [00:00, 35.46it/s]\n",
      "16it [00:00, 35.23it/s]\n",
      "16it [00:00, 34.44it/s]\n",
      "16it [00:00, 34.56it/s]\n",
      "16it [00:00, 37.21it/s]\n",
      "16it [00:00, 37.09it/s]\n",
      "16it [00:00, 37.12it/s]\n",
      "16it [00:00, 36.15it/s]\n",
      "16it [00:00, 32.73it/s]\n",
      "16it [00:00, 32.78it/s]\n",
      "16it [00:00, 35.10it/s]\n",
      "16it [00:00, 33.10it/s]\n",
      "16it [00:00, 37.44it/s]\n",
      "16it [00:00, 37.89it/s]\n",
      "16it [00:00, 37.06it/s]\n",
      "16it [00:00, 35.98it/s]\n",
      "16it [00:00, 33.18it/s]\n",
      "16it [00:00, 37.15it/s]\n",
      "16it [00:00, 33.12it/s]\n",
      "16it [00:00, 33.39it/s]\n",
      "16it [00:00, 36.37it/s]\n",
      "16it [00:00, 36.96it/s]\n",
      "16it [00:00, 35.43it/s]\n",
      "16it [00:00, 36.42it/s]\n",
      "16it [00:00, 33.31it/s]\n",
      "16it [00:00, 33.25it/s]\n",
      "16it [00:00, 36.18it/s]\n",
      "16it [00:00, 37.18it/s]\n",
      "16it [00:00, 37.18it/s]\n",
      "16it [00:00, 36.40it/s]\n",
      "16it [00:00, 37.39it/s]\n",
      "16it [00:00, 37.21it/s]\n",
      "16it [00:00, 36.86it/s]\n",
      "16it [00:00, 37.54it/s]\n",
      "16it [00:00, 36.77it/s]\n",
      "16it [00:00, 37.67it/s]\n",
      "16it [00:00, 36.70it/s]\n",
      "16it [00:00, 37.87it/s]\n",
      "16it [00:00, 36.92it/s]\n",
      "16it [00:00, 35.59it/s]\n",
      "16it [00:00, 36.21it/s]\n",
      "16it [00:00, 33.46it/s]\n",
      "16it [00:00, 34.64it/s]\n",
      "16it [00:00, 35.59it/s]\n",
      "16it [00:00, 37.02it/s]\n",
      "16it [00:00, 36.66it/s]\n",
      "16it [00:00, 37.35it/s]\n",
      "16it [00:00, 36.98it/s]\n",
      "16it [00:00, 36.54it/s]\n",
      "16it [00:00, 33.34it/s]\n",
      "16it [00:00, 34.12it/s]\n",
      "16it [00:00, 36.86it/s]\n",
      "16it [00:00, 35.63it/s]\n",
      "16it [00:00, 37.31it/s]\n",
      "16it [00:00, 36.90it/s]\n",
      "16it [00:00, 36.86it/s]\n",
      "16it [00:00, 37.31it/s]\n",
      "16it [00:00, 36.93it/s]\n",
      "16it [00:00, 34.80it/s]\n",
      "16it [00:00, 30.37it/s]\n",
      "16it [00:00, 32.55it/s]\n",
      "16it [00:00, 35.29it/s]\n",
      "16it [00:00, 34.29it/s]\n",
      "16it [00:00, 36.60it/s]\n",
      "16it [00:00, 37.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate fiducial data for MOPED compression\n",
    "fiducial_params = {'h': 0.7, 'Oc': 0.3, 'Ob': 0.04}\n",
    "n_fiducial_samples = 80\n",
    "\n",
    "fiducial_param_samples = []\n",
    "fiducial_cls_samples = []\n",
    "for n in range(n_fiducial_samples):\n",
    "    sim = lensing_cls_sim(fiducial_params)\n",
    "    fiducial_param_samples.append(sim['params'])\n",
    "    fiducial_cls_samples.append(sim['cls'])\n",
    "fiducial_param_samples = np.array(fiducial_param_samples)\n",
    "fiducial_cls_samples = np.array(fiducial_cls_samples)\n",
    "\n",
    "# print(\"Fiducial Params:\\n\", fiducial_param_samples)\n",
    "# print(\"Fiducial Cls:\\n\", fiducial_cls_samples)\n",
    "\n",
    "with open('moped_fiducial_cls.pkl', 'wb') as f:\n",
    "    pickle.dump({'params': fiducial_param_samples, 'cls': fiducial_cls_samples}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843ac9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoped(param_samples, cls_samples, fiducial_cls, perturbed_cls):\n",
    "    '''\n",
    "    fiducial_cls used for covariance computation\n",
    "    perturbed_cls used for mean derivative computation\n",
    "    '''\n",
    "    return {'params': param_samples, 'compressed_cls': compressed_cls}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170c877b",
   "metadata": {},
   "source": [
    "# Density Estimation\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
